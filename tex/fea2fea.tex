% TEX program = xelatex
%%
%% This is file `sample-sigconf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `sigconf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%% The first command in your LaTeX source must be the \documentclass command.

\documentclass[sigconf]{acmart}
\newcommand{\B}{\fontseries{b}\selectfont}
\usepackage[ruled,linesnumbered]{algorithm2e}
\usepackage{svg}
\usepackage{amsmath}
\usepackage{epstopdf}
\usepackage{tikz}  
\usetikzlibrary{arrows.meta,positioning,calc}
\usepackage{tkz-graph}
\usepackage{graphicx}  
%% NOTE that a single column version is required for 
%% submission and peer review. This can be done by changing
%% the \doucmentclass[...]{acmart} in this template to 
%% \documentclass[manuscript,screen]{acmart}
%% 
%% To ensure 100% compatibility, please check the white list of
%% approved LaTeX packages to be used with the Master Article Template at
%% https://www.acm.org/publications/taps/whitelist-of-latex-packages 
%% before creating your document. The white list page provides 
%% information on how to submit additional LaTeX packages for 
%% review and adoption.
%% Fonts used in the template cannot be substituted; margin 
%% adjustments are not allowed.

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmcopyright}
\copyrightyear{2021}
\acmYear{2021}
%\acmDOI{10.1145/1122445.1122456}

%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[KDD'21]{}{14-18 August}{ Singapore}
%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
 % June 03--05, 2018, Woodstock, NY}
%\acmPrice{15.00}
%\acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Feature Augmentation on Small Graphs}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.

\author{Jiaqing Xie}
\affiliation{%
  \institution{University of Edinburgh}
  %\streetaddress{1 Th{\o}rv{\"a}ld Circle}
  \city{Edinburgh}
  \country{United Kingdom}}
%\email{s2001696@ed.ac.uk}

\author{Rex Ying}
\affiliation{%
  \institution{Stanford University}
  %\city{Rocquencourt}
  \country{USA}
}


%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
%\renewcommand{\shortauthors}{Trovato and Tobin, et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
 
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{datasets, neural networks, gaze detection, text tagging}

%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.


%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}

\begin{figure}[!h]
    \centering
    \includegraphics[width = 0.45\textwidth]{fig/LesMiserables.eps}
    \caption{Question: In LesMiserables dataset, if already know the character Valjean has 35 neighbours(degrees), can we predict the feature PageRank of Valjean through graph neural network?) }
\end{figure}

\begin{figure*}
    \centering
    \includegraphics[width=0.8\textwidth]{output.eps}
    \caption{Baseline model for feature mutual prediction task}
\end{figure*}


 Graph neural networks(GNN) are widely used in node classification, graph classification, link prediction problems and graph embedding extractions. Graph neural networks have broad application scenarios including knowledge graph, social network, computer network and also recommender systems. Better learning of graph structures and comprehensive learning of features' relationship on those tasks and application scenarios based on GNN might enable researchers to make predictions precisely. In a regular graph classification task, adjacency matrix and feature matrix are considered as the default inputs of a graph neural network. For example, benchmark Cora dataset has an input feature matrix F  $\in \mathbb{R}^{2708 \times1433}$ where it represents 2708 papers with 1433 filtered words for each paper entry, and it also has an adjacency matrix A$ \in \mathbb{R}^{2708 \times2708}$, 
where each element represents the link information between each two papers (both 0-1 matrix). Graph properties are not added to our input but instead sometimes we analyze them alone by taking each property into consideration. By giving an directed or undirected graph, we can easily get the property of degree of each node or the nodes' clustering coefficients by counting edges but we do not predict them at all, which means the prediction objects are always the nodes' or graph's classes. We do not know if adding those properties can help improve node or graph classification accuracy or not, or if there exists a strong relationship between features or not. 

In this paper, we build models that are based on various kinds of graph convolutional layer types, including Graph Attention Netowrk(GAT), GraphSAGE, Graph Isomorphic Network(GIN) and Graph Convolutional Network(GCN) with multi-layer perceptron to perform mutual feature predictions instead of node classifications on the benchmark dataset to see the relationship between features. The features that selected for our research are  constant feature, node degrees, clustering coefficient, average path length and pagerank. We choose Planetoid, TUDataset, Reddit and PPI dataset for the research and also include the random generated graph for further validation. If one feature predict the other feature precisely, they are considered to be redundant to concatenated features' representation.  Finally we insert our artificial generated features to see which combination is the best, which is the feature augmentation period. Different datasets may have different feature combinations to augment their feature representations.   



We design totally different experiments instead of traditional graph classification or link prediction problems in order to know features' relationship: (i) feature to feature prediction (ii) concatenated features to feature prediction (iii) original features with concatenated features to feature prediction(augmentation). Our task is to know (i) if GNN can perform these tasks well, including our own models, to test if GNN is powerful at predicting everything related (ii) of which features are closely related according our predictions, high value means redundant (iii) do augmentation really works on mutual feature prediction (iv) to see if node tasks and graph tasks are both effective to augmentation.

\section{Related Works}



\subsection{Graph Representation Learning}

\subsection{Feature Importance}

\section{Methods}


\subsection{Baseline Graph embedding methods}
\paragraph{GCN} GCN is the abbreviation of \textbf{Graph Convolutional Network}\cite{kipf2016semi}. It derives from  signal processing domain which takes the advantage of Laplacian matrix representation and filters by applying fourier transforms to graph signals. Here's the propagation rule of a multi-layer GCN: 
\begin{equation}
\setlength\abovedisplayskip{5pt}%shrink space
\setlength\belowdisplayskip{5pt}
X^{\left(l+1\right)} = \sigma\left(\widetilde D^{-\frac{1}{2}} \widetilde A \widetilde D^{-\frac{1}{2}} X^{(l)} W^{(l)}\right)\\
\end{equation}
$X^{(l)} \in  \mathbb R^{N_{l} * D_{l}}$ is the input of layer $(l+1)$, where $X^{(0)}$ is the initial feature matrix of the graph. $\widetilde D_{ii} = \sum_{j}\widetilde A_{ij}$ is a learnable degree matrix and $W^{(l)}$ is a learnable weight matrix. $\widetilde A = I_{n} + A$ is an adjacency matrix with self-connection and $I_{n}$ is the identity matrix. It performs normalization to alleviate gradient vanishing problems.  GCN has the time complexity of $O(V)$ where V is equal to number of nodes if adjacency matrix $\widetilde 
A$ is sparse. GCN embedding is more efficient in small graphs than large graphs. It's a transductive method which takes the whole graph into account. However, it's not influential when encoding small graphs that we want to explore.

\paragraph{GraphSAGE}  GraphSAGE is an inductive graph representation learning method\cite{hamilton2017inductive}, which performs better than transductive methods such as GCN on large graphs. According to small graphs, they might show no difference on running time. The iteration of graph representation is computed by the aggregation of neighbourhood message of a given node, not considering all the edges in the graph. It concatenates itself to the message passing from its neighbours at k-th iteration with an MLP:
\begin{equation}
\setlength\abovedisplayskip{5pt}%shrink space
\setlength\belowdisplayskip{5pt}
h_{v}^{(k)} = \sigma\left(\mathbf{W}^{(k)} \cdot \textup{\small CONCAT} \left(h_{v}^{(k-1)},h_{\mathcal N\left(v\right)}^{(k)}\right)\right), v \in \mathcal V\\ 
\end{equation}
Here $h_{v}^{(k)}$ is the representation of node v at k-th iteration. $h_{\mathcal N\left(v\right)}^{(k)}$ is the aggregation of node v's neighbours at k-th iterarion. $\mathbf{W}^{(k)}$ is the trainbale weight matrix at k-th iteration. $\sigma\left( \cdot \right)$ is the activation function. The nearby nodes show similar output structures while disparate nodes have totally different representations\cite{hamilton2017inductive}. Aggregation methods include mean, maxpooling, sum and LSTM aggregator\cite{hamilton2017inductive}.

\paragraph{GIN}  GIN is the abbreviation of \textbf{Graph Isomorphism Network}\cite{xu2018powerful}. Experiments show that GIN is as powerful as WL test, which can differentiate two different graph structures and identify two isomorphic graphs. Consider the propagation rule of GIN:
\begin{equation}
\setlength\abovedisplayskip{5pt}%shrink space
\setlength\belowdisplayskip{5pt}
h_{v}^{(k)} = \textup{MLP}^{(k)}\left( \left( 1 + \varepsilon^{(k)}\right) \cdot h_{v}^{(k-1)} + \sum\nolimits_{u \in \mathcal N(v)} h_{u}^{(k-1)}\right)  \\
\end{equation}
Here $h_{v}^{k}$ is the representation of node v at k-th iteration. It can be also written by $f\circ\varphi$. {\sc Gin} uses MLP to learn $f$ and $\varphi$. Injective function avoids two multisets generate the same embeddings from GNN. Meanwhile, similar graph embeddings have similar embeddings in GIN. Output module is essential in GIN, which includes a concatenation process and a readout module. Readout module is sum in GIN, while max and mean dissatisfy injective property. GraphSAGE embedding with mean aggregator performs as well as GIN. They have great performance on graph but not generally on node classification.

\paragraph{GAT} GAT is the abbreviation of \textbf{Graph Attention Network}\cite{velivckovic2017graph}. It takes the advantage of attention mechanism, which is first proposed in a transformer architecture in NLP domain. The attention weight between node i and one of its neigbours node j can be expressed as $\alpha_{ij}$: 
\begin{equation}
\setlength\abovedisplayskip{5pt}%shrink space
\setlength\belowdisplayskip{5pt}
\alpha_{ij} = \frac{\textup{exp}\left(\textup{LeakyReLU}\left(\vec{\mathbf{a}}^{T}[\mathbf{W}\vec{h_{i}}||\mathbf{W}\vec{h_{j}}]\right)\right)}{\sum_{k \in \mathcal N\left(i\right)}\textup{exp}\left(\textup{LeakyReLU}\left(\vec{\mathbf{a}}^{T}[\mathbf{W}\vec{ h_{i}}||\mathbf{W} \vec{h_{k}}]\right)\right)}\\
\end{equation}
Here it performs softmax operation. $\vec{h_{i}^{'}}$ is the node embedding of node i.$\vec{h_{j}^{'}} \in \mathbb{R}^{F'}$
is representation of one of node i's neighbours. $\vec{\mathbf{a}}^{T} \in \mathbb{R}^{2F'}$ is parameterized weight vector. LeakyReLU is a non-linear activation function. Output feature is $\vec{h_{i}'}$:
\begin{equation}
\setlength\abovedisplayskip{5pt}%shrink space
\setlength\belowdisplayskip{5pt}
\vec{h_{i}'} = \sigma\left(\sum_{j \in \mathcal N_{i}}\alpha_{ij}\mathbf{W}\vec{h_{j}}\right)\\
\end{equation}
It'a a linear combination with non-linearity $\sigma(\cdot)$. The combination method of multi-head attention in GAT is take averaging result. The advantage of GAT is that we can perform inductive learning without knowing everything about the whole graph.

\subsection{Feature to Feature Prediction}





\begin{algorithm}
\caption{Get Matrix of Feature Mutual Relationship}
\KwIn{Graph $\mathcal G$($\mathcal V, \mathcal E$);  node property length $\mathcal K$; Input feature matrix 
  $\mathbf{x}_{\mathcal{G}}\in\mathbb{R}^{|\mathcal{V}|\times \mathcal{K}} $, second dimension order is constant 
  feature, degree, clustering coefficient, pagerank and average path length or more features; 
  model architecture $\mathcal M$, including GNN embedding(GCN, GraphSAGE, GIN, GAT) with MLP model; 
  metrics $\mathcal P$ }
\KwOut{ Feature mutual relationship matrix $\mathcal R \in \mathbb{R}^{\mathcal K \times \mathcal K}$}
$\mathcal R \gets 0$\\
\For{$i \gets 1$ \textbf{to} $\mathcal K$}{
    $\textbf{I}_{\mathcal G} \gets \textbf{x}_{\mathcal G}(:,i)$\\

    \For{ $j \gets 1$ \textbf{to} $\mathcal K$
    }{

    $\textbf{O}_{\mathcal G} \gets \textup{Bin}\left(\textbf{x}_{\mathcal G}(:,j)\right)$\\
    $\textbf{O}^{'}_{\mathcal G} \gets$ $\mathcal{M}\left(\textbf{I}_{\mathcal G}, \textbf{O}_{\mathcal G}\right)$\\
    $\mathcal{R}(i,j) \gets \mathcal P(\textbf{O}^{'}_{\mathcal G},\textbf{O}_{\mathcal G} )$
    }
    \If{i == $\mathcal{K}$}{
      \Return{$\mathcal R$}\;
      }
}
\end{algorithm}

Given a graph $\mathcal G$($\mathcal V, \mathcal E$). We want to achieve the desired feature 
matrix $\textbf{x}_{\mathcal G} \in  \mathbb{R}^{|\mathcal{V}|\times \mathcal K}$ from $\mathcal G$ which 
requires an adjacency matrix $\mathcal A_{g} \in \mathbb{R}^{|\mathcal{V}|*|\mathcal{V}|}$ of $\mathcal G$. 
Suppose that we choose five of all graph features to show exploratory results, which are constant feature (Cons), 
degree (Deg), clustering coefficient (Clu), average path length (AvgLen) and PageRank (PR).

Constant feature of one node u $\mathop{Cons}\limits_{u \in \mathcal{V}}(u)$ is given by c, 
where c is equal to a constant value. We set to 1 in this paper for normalization. 
Degree of node u is equal to $\mathop{Deg}\limits_{u \in \mathcal{V}}(u)$ is equal to the number of node u's neighbours. 
Clustering coefficient of node u $\mathop{Clu}\limits_{u \in \mathcal{V}}(u)$ = $\frac{2e_{jk}}{k_{i} * (k_{i} - 
1)}$, where $j,k \in \mathcal{V}$,$e_{jk}$ represents the total possible edges between node u's neighbours and
$k_{i}$ is the number of node u's neighbours. $\mathop{PR}\limits_{u \in \mathcal{V}}(u)$ is given by:
\begin{equation}
  \mathop{PR}\limits_{u \in \mathcal{V}}(u) = \frac{1-q}{|\mathcal V|} + q \times \sum_{v \in {\mathcal N} 
  (u)}\frac{\mathop{PR}(v)}{\mathcal L(v)}\\
\end{equation}
where $\mathcal N(\cdot)$ means the node u's neighbours and $\mathcal L(\cdot)$ means the outbound link numbers of the node u
to its neighbours. In the traditional analysis of pagerank algorithm, the object is directed graph. However we can treat
undirected graph as bidirectional graph, so outbound links is equal to the edge numbers from node u to its neigbours.
q is the residual probability, which is defualt as 0.85. We treat it as a hyper-parameter. $\mathop{Avglen}
\limits_{u \in \mathcal{V}}(u)$ is calculated by:
\begin{equation}
  \mathop{Avglen}
\limits_{u \in \mathcal{V}}(u) = \frac{1}{\mathcal V '}\sum_{v \in {\mathcal V}\neq u} I(u,v) * d_{min}(u,v)\\
\end{equation}
where $I(u,v)$ indicates whether there's a path from node u to node v. If node v is reachable according to node u,
then $I(u,v)$ is equal to 1 else it is 0. $\mathcal V'$ is equal to the total number of reachable nodes for node u,
more specifically, $\mathcal V'$ = $\sum_{v \in {\mathcal V}\neq u} I(u,v) $. $d_{min}(u,v)$ determines the shortest path
length from node u to node v. 

The algorithm 1 shows a complete process of how to construct a matrix for mutual feature prediction. Suppose that
we have successfully compute a feature matrix $\textbf{x}_{\mathcal G} \in  \mathbb{R}^{|\mathcal{V}|\times \mathcal K}$.
Specifically $\mathcal K$ is equal to 5 here since five of graph features are being researched. The features have been indexed. Suppose we have constructed 
a model $\mathcal M$ which uses the baseline graph embedding methods together with multi-layer perceptrons. 
Given a feature, we can find its column index from $\textbf{x}_{\mathcal G}$ and extract them by only taking that column,
such as constant feature: $\textbf{x}_{\mathcal G}(:,1)$ and average path length: $\textbf{x}_{\mathcal G}(:,4)$. We need to extract two columns, one
for input $\textbf{I}_{\mathcal G}$ and the other for output $\textbf{O}_{\mathcal G}$. We need to set bins for the output since the main task is classification not regression task. After outputs are classified into 
each own class, we use that constructed model $\mathcal M$ to predict output  $\textbf{O}^{'}_{\mathcal G}$. We use a metrics to evaluate the result between 
$\textbf{O}^{'}_{\mathcal G}$ and $\textbf{O}_{\mathcal G}$, which is the classification accuracy here. 
Finally we will achieve a feature mutual relationshiop matrix $\mathcal R$.
The binning results on the datasets that we have chosen are dicussed in part 4.3. We should be careful that constant feature can be only 
taken as input but not as output. It is the most basic feature for all graphs which could not lead to a self classification problem.



\subsection{Feature Concatenation}
\begin{algorithm}
  \caption{Get Matrix of Feature Mutual Relationship}
  \KwIn{Graph $\mathcal G$($\mathcal V, \mathcal E$);  node property length $\mathcal K$; Input property matrix   $\mathbf{x}_{\mathcal{G}}\in\mathbb{R}^{|\mathcal{V}|\times \mathcal{K}} $, second dimension order is constant feature, degree, clustering coefficient, pagerank and average path length; model architecture $\mathcal M$, including GNN embedding(GCN, GraphSAGE, GIN, GAT) with MLP model; metrics $\mathcal P$ }
  \KwOut{ Feature mutual relationship matrix $\mathcal R \in \mathbb{R}^{\mathcal K \times \mathcal K}$}
  $\mathcal R \gets 0$\\
  \For{$i \gets 1$ \textbf{to} $\mathcal K$}{
      $\mathcal R(i,i) \gets 1$\\
      $\textbf{I}_{\mathcal G} \gets \textbf{x}_{\mathcal G}(:,i)$\\
      \If{i == $\mathcal{K}$}{
      \Return{$\mathcal R$}\;
      }
      \For{ $j \gets i + 1$ \textbf{to} $\mathcal K$
      }{
  
      $\textbf{O}_{\mathcal G} \gets \textup{Bin}\left(\textbf{x}_{\mathcal G}(:,j)\right)$\\
      $\textbf{O}^{'}_{\mathcal G} \gets$ $\mathcal{M}\left(\textbf{I}_{\mathcal G}, \textbf{O}_{\mathcal G}\right)$\\
      $\mathcal{R}(i,j), \mathcal{R}(j,i) \gets \mathcal P(\textbf{O}^{'}_{\mathcal G},\textbf{O}_{\mathcal G} )$
      }
  }
  \end{algorithm}
  We have a feature list $\mathcal F$ = $\{ \mathcal F_{1}, \mathcal F_{2},..., \mathcal F_{n}\}$, which contains n 
  features of the graph. The value $\mathcal R(i,j)$ in relationship matrix indicates if $ \mathcal R_{i}$ and
  $\mathcal R_{j}$ are predictable to each other mutually. Moreover, we want to add additional features to $\mathcal R_{i}$
  to see if this will predict $\mathcal R_{j}$ more accurately. However we cannot add features arbitrarily since adding similar features might 
  bring about redundancy in data. In this paper we provide three main ideas for concatenating features.
  
\paragraph{simple concatenation}
If we 







\paragraph{Neural Tensor Network}
Apart from concatenating two or more features directly, we use try to use neural tensor network,
which is presented by:

\begin{equation}
  g_{new} = u^{T} a(f_{1}^{T} W_{R} f_{2} + V [ f_{1}, f_{2} ] + b) \\ 
\end{equation}




\paragraph{SkipLast}We

We now go through the algorithm 2.




\subsection{model architectures}


\section{Experimemts}



\begin{figure*}[h]
\centering
\begin{center}
 \hspace*{-1.5in}
    \includegraphics[width=0.27\linewidth]{fig/Aver_path_len_planetoid_vp.eps}
    \includegraphics[width=0.27\linewidth]{fig/Clustering_coefficient_planetoid_vp.eps}
    \includegraphics[width=0.27\linewidth]{fig/Degree_planetoid_vp.eps}
    \includegraphics[width=0.27\linewidth]{fig/Pagerank_planetoid_vp.eps}
    \hspace*{-1.5in}
    \caption{Violin plots of graph properties on planetoid datasets}
\end{center}
\end{figure*}



We compute the feature matrices for each graph. We implement four graph embedding methods: GIN, GCN, GraphSAGE and GAT on graph feature inputs to predict features' relationship matrices, which is regarded as the traditional GNN result on new tasks. We also compare them with the added GNN blocks that we have designed, as the comparison of GNN models in order to interpret Graph Neural Network's robustness. We've also handled with some important issues in the following parts. Typical datasets that meet the requirement of small graphs are listed in 4.1. Coding environment settings and parameter settings are detailed in 4.2. 

\subsection{Datasets}
In our preliminary experiments, we choose ten small graph-based datasets. A generation of supervised graph is also included. However, according to the data that mentioned in original GraphSage
paper, which are the aimed large graph, we do not care about them at this stage.
\paragraph{Planetoid}
Planetoid datasets are citation datasets including {\sc Cora}, {\sc CiteSeer} and {\sc PubMed}. One node in the graph represents each paper. Edge index means there is a citation link between two papaers and nodes in the graph are undirectly linked. The feature matrix of each dataset is given by sparse bags-of-words vectors. It's one-hot encoding which is at a lower level embedding space. In  {\sc Cora} dataset, there are 2708 nodes with 1433 features and 5429 edges. In {\sc CiteSeer} dataset, there are 3327 nodes with 3703 features and 4732 edges. In {\sc PubMed} dataset, there are 19717 nodes with 500 features and 44338 edges. In this work, we choose all of three datasets to show the generalization effects, which is that firstly we test on {\sc Cora} dataset, then we test on {\sc CiteSeer} and {\sc PubMed}.

\paragraph{TUDataset}
TUDataset is a collection of benchmark datasets for learning graph representations, which gathers numerous domains and is authored by different experts. More specifically, it includes data from small molecules, bioinformatics, social networks, computer vision and other synthetic kernels. In this work, we choose two datasets  {\sc Proteins} and {\sc Enzymes} from Bioinformatics domain. In {\sc Enzymes} dataset, there are 600 graphs with 6 classes. Each graph has an average edges of 62.1 and an average edges of 32.6. In {\sc Proteins} dataset, there are 1113 graphs with 2 classes. Each graph has an average egdes of 39.1 and an average edges of 72.8. We will make more experiments on other datasets, such as {\sc Zinc} and {\sc Qm9} from molecular datasets or {\sc Reddit-Binary} and {\sc Reddit\_Threads} from social networks. 

\paragraph{PPI}
Protein to Protein Interaction(PPI) network is generally used in biochemistry, containing positional gene sets, motif gene sets and immunological signatures as features (50 in total) and gene ontology sets as labels (121 in total). It is mentioned in this paper. 

\subsection{Experiment Set-up}
We've mentioned four graph embedding methods, which are GCN, GraphSAGE, GIN and GAT. 
Our baseline model is constructed by using graph embeddings and followed by MLPs. We take 

In single feature to feature prediction tasks, we set up a 2-layer GNN with each layer followed by \textit{batch norm}
layer, activated by\textit{relu function} and using \textit{dropout} method. Especially  

GPU settings: GeForce RTX 2060 Super



Add parameter setting and coding environment here:\\

We mainly use Pytorch framework together with torch\_geometric API for building our model architecture.  In the single feature to single feature task, our default model consists of two GNN blocks and a MLP block with two linear layers. We both have classification tasks for feature prediction and also regression tasks. The metrics for classification task is the accuracy score and macro-F1 score. Input dimension is set to 1 which is input\_channel parameter. 

\subsection{Feature properties of graphs}
We choose five of all graph features to show exploratory results, which are constant feature(Cons), degree(Deg), clustering coefficient(Clu), average path length(AvgLen) and PageRank(PR).

Given a graph $\mathcal{G}$($\mathcal{V}$, $\mathcal{E}$), constant feature of one node u $\mathop{Cons}\limits_{u \in \mathcal{V}}(u)$ is given by c, where c is equal to a constant value. We set to 1 in this project for normalization. Degree of node u $\mathop{Deg}\limits_{u \in \mathcal{V}}(u)$ is equal to the number of node u's neighbours. Clustering coefficient of node u $\mathop{Clu}\limits_{u \in \mathcal{V}}(u)$ = $\frac{2e_{jk}}{k_{i} * (k_{i} - 1)}$, where $j,k \in \mathcal{V}$,$e_{jk}$ represents the total possible edges between node u's neighbours and $k_{i}$ is the number of node u's neighbours.

Having the prerequisite of these basic knowledge, we compute the feature matrix for each dataset. The feature matrix serves to $\mathcal{R}^{|V|*5}$. We should bear in mind that this feature matrix is both used for input and output, as described in Algorithm 1. One important step before fed into training session is to measure the distribution of each feature. Just as imbalanced data is not favoured in the graph or node classification tasks, we observe all the feature distributions and illustrate potential issues among all datasets.


\paragraph{binning methods} Suppose the clustering coefficient is the feature that we want to predict. Clustering coefficient is a non-integer value as we've discussed before. We firstly take it as the classification problem. Binning methods is then used to identify the     Therefore we set bins in order to change the outputs to integers. Figure 2 shows the graph properties on planetoid datasets with violin plots. The property order is: degree, clustering coefficient, pagerank and average path length. Density distribution is very different between each dataset and each property as well. Generally, we set 4-8 bins. Too large bins may lead to a more sparse confusion matrix while too small bins may lead to over concentration on one class. So the number of bins should be not too big nor too small and the number of 4 to 8 satisfies this condition. Specifically, we take the Cora dataset as an example to illustrate the point. Figure 3 shows the example of the property's density distribution on Cora dataset with distplots. We can set 4 bins for Degree, Clustering\_coefficient, Pagerank and Aver\_path\_len according to the values that shown on the x axis of the distplots. 

Through the violin plot we find that most of the clustering coefficients are 0 in planetoid datasets. Therefore we treat 0 as a single class. When setting bins for all the data, we remove all the zeros since it's not reasonable to set bins where many of the bin values are 0. Specifically in figure 3, we see that in Cora dataset, the density function concentrates on the 0.00 since most of nodes(1126 out of 2708) in Cora dataset has a zero clustering coefficient. Similarly 2104 out of 3327 nodes has a zero clustering coefficient in PubMed dataset while 14899 out of 19717 nodes have a zero clustering coefficient in Citeseer dataset.








\subsection{Feature to feature prediction results}



\begin{table*}[htb]
  \caption{Feature to Feature Prediction Results on Citation Datasets (bins = 6)}
  \label{tab:commands}
  \begin{tabular}{*{13}{c}} \toprule
{Aim}  & \multicolumn{4}{c}{{\sc Cora}} & \multicolumn{4}{c}{{\sc CiteSeer}} & \multicolumn{4}{c}{{\sc PubMed}}\\
\cmidrule(lr){2-5}\cmidrule(lr){6-9}\cmidrule(lr){10-13}
& GCN & GIN & SAGE & GAT & GCN & GIN & SAGE & GAT & GCN & GIN & SAGE & GAT \\ \hline
{1 -> 2} & 0.509 & \B 1.000 & 0.213 & 0.202 & 0.578 & \B 0.999 & 0.379 & 0.379 & 0.653 & \B0.998  & 0.478 & 0.478\\
{1 -> 3} & 0.523 & \B 0.533 & 0.461 & 0.461 & 0.673 & \B 0.707 & 0.658 & 0.658 & \B 0.789 & 0.797  & 0.780 & 0.780 \\
{1 -> 4} & 0.639 & \B 0.756 & 0.160 & 0.160 & 0.706 & \B 0.662 & 0.201 & 0.185 & \B 0.664 & 0.526 & 0.161 & 0.143\\
{1 -> 5} & 0.357 & \B 0.384 & 0.169 & 0.169 & 0.406 & \B 0.495 & 0.182 & 0.170 & 0.300 & \B 0.410 & 0.166 & 0.171\\
{2 -> 3} & 0.550 & 0.542 & \B 0.548 & 0.506 & 0.704 & 0.706 & \B 0.718 & 0.674 & 0.800 & \B 0.802 & 0.799 & 0.781\\
{2 -> 4} & 0.573 & \B 0.792 & 0.750 & 0.392 & 0.619 & \B 0.754 & 0.756 & 0.350 & 0.532 & \B 0.617 & 0.609 & 0.344\\
{2 -> 5} & 0.420 & 0.435 & \B 0.440 & 0.340 & 0.532 & \B 0.546 & 0.544 & 0.497 & 0.422 & \B 0.450 & 0.426 & 0.338\\
{3 -> 2} & 0.423 & \B 1.000 & 0.504 & 0.285 & 0.582 & \B 0.999 & 0.609 & 0.489 & 0.638 & \B 0.998 & 0.568 & 0.475\\
{3 -> 4} & 0.427 & \B 0.695 & 0.403 & 0.199 & 0.530 & \B 0.554 & 0.427 & 0.290 & 0.441 & \B 0.565 & 0.394 & 0.254\\
{3 -> 5} & 0.286 & \B 0.310 & 0.263 & 0.219 & \B 0.409 & 0.383 & 0.387 & 0.356 & 0.255 & \B 0.314 & 0.285 & 0.187\\
{4 -> 2} & 0.308 & \B 1.000 & 0.311 & 0.223 & 0.297 & \B 1.000 & 0.414 & 0.383 & 0.585 & \B 1.000 & 0.482 & 0.478\\
{4 -> 3} & 0.490 & \B 0.538 & 0.486 & 0.461 & 0.672 & \B 0.698 & 0.665 & 0.658 & 0.796 & \B 0.805 & 0.780 & 0.780\\
{4 -> 5} & 0.215 & \B 0.421 & 0.266 & 0.185 & 0.249 & \B 0.401 & 0.238 & 0.282 & 0.313 & \B 0.449 & 0.192 & 0.174\\
{5 -> 2} & 0.409 & \B 1.000 & 0.499 & 0.228 & 0.542 & \B 0.999 & 0.672 & 0.419 & 0.648 & \B 0.996 & 0.704 & 0.478\\
{5 -> 3} & 0.508 & \B 0.538 & 0.498 & 0.460 & 0.683 & \B 0.701 & 0.679 & 0.658 & \B 0.798 & 0.797 & 0.788 & 0.780\\
{5 -> 4} & 0.450 & \B 0.741 & 0.490 & 0.202 & 0.530 & \B 0.645 & 0.603 & 0.300 & 0.551 & \B 0.542 & 0.573 & 0.152\\
\bottomrule
  \end{tabular}
\end{table*} 

\begin{table*}[htb]
  \caption{Feature to Feature Prediction Results on TUDatasets (bins = 6)}
  \label{tab:commands}
  \begin{tabular}{*{13}{c}} \toprule
{Aim}  & \multicolumn{4}{c}{{\sc Proteins}} & \multicolumn{4}{c}{{\sc Enzymes}} & \multicolumn{4}{c}{{\sc NCI1}}\\
\cmidrule(lr){2-5}\cmidrule(lr){6-9}\cmidrule(lr){10-13}
& GCN & GIN & SAGE & GAT & GCN & GIN & SAGE & GAT & GCN & GIN & SAGE & GAT \\ \hline
{1 -> 2} & 0.560 & \B 0.662 & 0.469 & 0.469 & 0.630 & \B 0.881 & 0.418 & 0.418 & 0.833 & \B 0.916 & 0.406 & 0.401\\
{1 -> 3} & 0.299 & \B 0.436 & 0.202 & 0.202 & 0.315 & \B 0.344 & 0.262 & 0.261 & / & / & / & / \\
{1 -> 4} & 0.645 & \B 0.648 & 0.170 & 0.170 & 0.597 & \B 0.639 & 0.181 & 0.169 & 0.630 & \B 0.636 & 0.172 & 0.169\\
{1 -> 5} & \B 0.182 & 0.171 & 0.171 & 0.171 & 0.169 & \B 0.200 & 0.173 & 0.169 & 0.217 & \B 0.223 & 0.172 & 0.170\\
{2 -> 3} & 0.435 & 0.387 & \B 0.454 & 0.312 & 0.397 & 0.359 & \B 0.407 & 0.314 & / & / & / & / \\
{2 -> 4} & 0.622 & 0.699 & \B 0.722 & 0.239 & 0.623 & 0.679 & \B 0.702 & 0.249 & 0.689 & 0.705 & \B 0.740 & 0.304\\
{2 -> 5} & \B 0.212 & 0.184 & 0.200 & 0.171 & 0.233 & 0.216 & \B 0.245 & 0.173 & \B 0.237 & 0.222 & 0.235 & 0.177\\
{3 -> 2} & 0.537 & \B 0.652 & 0.482 & 0.469 & 0.569 & \B 0.788 & 0.544 & 0.418 & / & / & / & / \\
{3 -> 4} & \B 0.575 & 0.465 & 0.467 & 0.549 & \B 0.539 & 0.395 & 0.369 & 0.198 & / & / & / & / \\
{3 -> 5} & 0.227 & 0.196 & \B 0.254 & 0.228 & 0.197 & 0.169 & \B 0.203 & 0.169 & / & / & / & / \\	
{4 -> 2} & 0.430 & \B 0.662 & 0.446 & 0.469 & 0.240 & 0.336 & \B 0.458 & 0.418 & 0.100 & \B 0.827 & 0.417 & 0.341\\
{4 -> 3} & 0.247 & \B 0.330 & 0.278 & 0.219 & 0.269 & \B 0.305 & 0.270 & 0.262 & / & / & / & / \\
{4 -> 5} & 0.171 & \B 0.172 & 0.171 & 0.171 & 0.169 & \B 0.180 & 0.171 & 0.168 & 0.169 & \B 0.339 & 0.173 & 0.170\\
{5 -> 2} & 0.522 & \B 0.657 & 0.482 & 0.469 & 0.572 & \B 0.881 & 0.484 & 0.418 & 0.784 & \B 0.919 & 0.836 & 0.401\\
{5 -> 3} & 0.276 & 0.258 & \B 0.328 & 0.202 & 0.283 & 0.308 & \B 0.354 & 0.262 & / & / & / & / \\
{5 -> 4} & \B 0.579 & 0.453 & 0.395 & 0.170 & \B 0.542 & 0.430 & 0.360 & 0.170 & \B 0.629 & 0.534 & 0.588 & 0.180\\
\bottomrule
  \end{tabular}
\end{table*}
\subsubsection{GNN  performance}
We test the baseline GNN models to conclude their performance on \textit{Planetoid} datasets and \textit{TUDatasets}.
We need to ensure that model parameters are the same when testing each baseline model. 
The depth of the graph embedding method is set to 2 and the bin of number of classes is set to 6. For the first graph embedding layer,
input dimension is 1 and output dimension is 256, while for the second graph embedding layer input dimension is 256 and output dimension is 64.
A relu unit and dropout layer is followed by each graph embedding layer. After the graph embedding block, multi-layer perceptrons are connected.
The input dimension is 64 for the MLP block while the output dimension is equal to value of bins, which is 6.  

According to the results on node datasets,
\textbf{GIN} performs the best among all baseline models. Of all 48 test tasks on three different dataset,
it takes up the 41 best test results. The results is based on the average of 10 times of testing.
We figure out that the feature \textit{degree} can be predicted precisely on \textit{Planetoid} dataset.
Predicting accuracy can reach 1.000 when testing {\sc Cora} dataset. For {\sc Citeseer} and {\sc PubMed} dataset,
accuracy on predicting \textit{degree} can reach 0.999 and 0.998. Feature \textit{Pagerank} can be predicted to a large extent for 
{\sc Cora} but more difficult for {\sc Citeseer} and {\sc PubMed}. Feature \textit{Clustering coefficient} can be predicted easily on
{\sc Citeseer} and {\sc PubMed} dataset while it's more difficult for \textbf{GIN} to predict \textit{Clustering coefficient}.
Overall it's not easy for all baseline models to predict average path length when model depth is shallow.


\begin{center}
  \begin{figure}[!htp]
  \begin{tikzpicture}[auto,node distance=1cm,thick,main node/.style={circle,fill=blue!20,draw,font=\sffamily\Large\bfseries}, scale=0.8]
  \node[main node] (A) at (0,5) {A};
  \node[main node] (B) at (2,6.5) {B};
  \node[main node] (E) at (4,5) {E};
  \node[main node] (C) at (0.5,3) {C};
  \node[main node] (D) at (3.5,3) {D};

  \path [->] (A) edge node { \tiny $-1.11$} (B);
  \path [->] (B) edge node { \tiny $2.11$} (E);
  \path [->] (A) edge node { \tiny $2.11$} (D);
  \path [->] (A) edge node {$4$} (C);
  \path [->] (B) edge node { \tiny$2.11$} (C);
  \path [->] let \p1=($(D)-(B)$),\n1={atan2(\y1,\x1)},\n2={180+\n1} in
   ($ (B.\n1)!2pt!90:(D.\n2) $) edge node { \tiny $2.11$} ($ (D.\n2)!2pt!-90:(B.\n1) $);
  \path [->] let \p1=($(A)-(E)$),\n1={atan2(\y1,\x1)},\n2={180+\n1} in
   ($ (E.\n1)!2pt!90:(A.\n2) $) edge node { \tiny $2.11$} ($ (A.\n2)!2pt!-90:(E.\n1) $);

  \path [->] let \p1=($(B)-(D)$),\n1={atan2(\y1,\x1)},\n2={180+\n1} in
   ($ (D.\n1)!2pt!90:(B.\n2) $) edge node { \tiny $-2.11$} ($ (B.\n2)!2pt!-90:(D.\n1) $);

  

  \path [->,bend left=10] (D) edge node {\tiny $5$} (C);
  \path [->,bend left=10] (C) edge node {$-5$} (D);
  \path [->] (E) edge node {$-3$} (D);
  \end{tikzpicture}
  \begin{tikzpicture}[auto,node distance=1cm,thick,main node/.style={circle,fill=blue!20,draw,font=\sffamily\Large\bfseries}, scale=0.8]
    \node[main node ] (A) at (0,5) {A};
    \node[main node ] (B) at (2,6.5) {B};
    \node[main node] (E) at (4,5) {E};
    \node[main node] (C) at (0.5,3) {C};
    \node[main node] (D) at (3.5,3) {D};
  
    \path [->] (A) edge node { \tiny $-1.11$} (B);
    \path [->] (B) edge node { \tiny $2.11$} (E);
    \path [->] (A) edge node { \tiny $2.11$} (D);
    \path [->] (A) edge node {$4$} (C);
    \path [->] (B) edge node { \tiny$2.11$} (C);
    \path [->] let \p1=($(D)-(B)$),\n1={atan2(\y1,\x1)},\n2={180+\n1} in
     ($ (B.\n1)!2pt!90:(D.\n2) $) edge node { \tiny $2.11$} ($ (D.\n2)!2pt!-90:(B.\n1) $);
    \path [->] let \p1=($(A)-(E)$),\n1={atan2(\y1,\x1)},\n2={180+\n1} in
     ($ (E.\n1)!2pt!90:(A.\n2) $) edge node { \tiny $2.11$} ($ (A.\n2)!2pt!-90:(E.\n1) $);
  
    \path [->] let \p1=($(B)-(D)$),\n1={atan2(\y1,\x1)},\n2={180+\n1} in
     ($ (D.\n1)!2pt!90:(B.\n2) $) edge node { \tiny $-2.11$} ($ (B.\n2)!2pt!-90:(D.\n1) $);
  
     \path [->] let \p1=($(C)-(E)$),\n1={atan2(\y1,\x1)},\n2={180+\n1} in
     ($ (E.\n1)!2pt!90:(C.\n2) $) edge node { \tiny $-2.11$} ($ (C.\n2)!2pt!-90:(E.\n1) $);
     \path [->] let \p1=($(E)-(C)$),\n1={atan2(\y1,\x1)},\n2={180+\n1} in
     ($ (C.\n1)!2pt!90:(E.\n2) $) edge node { \tiny $-1.11$} ($ (E.\n2)!2pt!-90:(C.\n1) $);
  
    \path [->,bend left=10] (D) edge node {\tiny $5$} (C);
    \path [->,bend left=10] (C) edge node {$-5$} (D);
    \path [->] (E) edge node {$-3$} (D);
    \end{tikzpicture}
    \begin{tikzpicture}[auto,node distance=1cm,thick,main node/.style={circle,fill=blue!20,draw,font=\sffamily\Large\bfseries}, scale=0.8]
      \node[main node ] (A) at (0,5) {A};
      \node[main node ] (B) at (2,6.5) {B};
      \node[main node] (E) at (4,5) {E};
      \node[main node] (C) at (0.5,3) {C};
      \node[main node] (D) at (3.5,3) {D};
    
      \path [->] (A) edge node { \tiny $-1.11$} (B);
      \path [->] (B) edge node { \tiny $2.11$} (E);
      \path [->] (A) edge node { \tiny $2.11$} (D);
      \path [->] (A) edge node {$4$} (C);
      \path [->] (B) edge node { \tiny$2.11$} (C);
      \path [->] let \p1=($(D)-(B)$),\n1={atan2(\y1,\x1)},\n2={180+\n1} in
       ($ (B.\n1)!2pt!90:(D.\n2) $) edge node { \tiny $2.11$} ($ (D.\n2)!2pt!-90:(B.\n1) $);
      \path [->] let \p1=($(A)-(E)$),\n1={atan2(\y1,\x1)},\n2={180+\n1} in
       ($ (E.\n1)!2pt!90:(A.\n2) $) edge node { \tiny $2.11$} ($ (A.\n2)!2pt!-90:(E.\n1) $);
    
      \path [->] let \p1=($(B)-(D)$),\n1={atan2(\y1,\x1)},\n2={180+\n1} in
       ($ (D.\n1)!2pt!90:(B.\n2) $) edge node { \tiny $-2.11$} ($ (B.\n2)!2pt!-90:(D.\n1) $);
    
       \path [->] let \p1=($(C)-(E)$),\n1={atan2(\y1,\x1)},\n2={180+\n1} in
       ($ (E.\n1)!2pt!90:(C.\n2) $) edge node { \tiny $-2.11$} ($ (C.\n2)!2pt!-90:(E.\n1) $);
       \path [->] let \p1=($(E)-(C)$),\n1={atan2(\y1,\x1)},\n2={180+\n1} in
       ($ (C.\n1)!2pt!90:(E.\n2) $) edge node { \tiny $-1.11$} ($ (E.\n2)!2pt!-90:(C.\n1) $);
    
      \path [->,bend left=10] (D) edge node {\tiny $5$} (C);
      \path [->,bend left=10] (C) edge node {$-5$} (D);
      \path [->] (E) edge node {$-3$} (D);
      \end{tikzpicture}
      \begin{tikzpicture}[auto,node distance=1cm,thick,main node/.style={circle,fill=blue!20,draw,font=\sffamily\Large\bfseries}, scale=0.8]
        \node[main node ] (A) at (0,5) {A};
        \node[main node ] (B) at (2,6.5) {B};
        \node[main node] (E) at (4,5) {E};
        \node[main node] (C) at (0.5,3) {C};
        \node[main node] (D) at (3.5,3) {D};
      
        \path [->] (A) edge node { \tiny $-1.11$} (B);
        \path [->] (B) edge node { \tiny $2.11$} (E);
        \path [->] (A) edge node { \tiny $2.11$} (D);
        \path [->] (A) edge node {$4$} (C);
        \path [->] (B) edge node { \tiny$2.11$} (C);
        \path [->] let \p1=($(D)-(B)$),\n1={atan2(\y1,\x1)},\n2={180+\n1} in
         ($ (B.\n1)!2pt!90:(D.\n2) $) edge node { \tiny $2.11$} ($ (D.\n2)!2pt!-90:(B.\n1) $);
        \path [->] let \p1=($(A)-(E)$),\n1={atan2(\y1,\x1)},\n2={180+\n1} in
         ($ (E.\n1)!2pt!90:(A.\n2) $) edge node { \tiny $2.11$} ($ (A.\n2)!2pt!-90:(E.\n1) $);
      
        \path [->] let \p1=($(B)-(D)$),\n1={atan2(\y1,\x1)},\n2={180+\n1} in
         ($ (D.\n1)!2pt!90:(B.\n2) $) edge node { \tiny $-2.11$} ($ (B.\n2)!2pt!-90:(D.\n1) $);
      
         \path [->] let \p1=($(C)-(E)$),\n1={atan2(\y1,\x1)},\n2={180+\n1} in
         ($ (E.\n1)!2pt!90:(C.\n2) $) edge node { \tiny $-2.11$} ($ (C.\n2)!2pt!-90:(E.\n1) $);
         \path [->] let \p1=($(E)-(C)$),\n1={atan2(\y1,\x1)},\n2={180+\n1} in
         ($ (C.\n1)!2pt!90:(E.\n2) $) edge node { \tiny $-1.11$} ($ (E.\n2)!2pt!-90:(C.\n1) $);
      
        \path [->,bend left=10] (D) edge node {\tiny $5$} (C);
        \path [->,bend left=10] (C) edge node {$-5$} (D);
        \path [->] (E) edge node {$-3$} (D);
        \end{tikzpicture}
        \begin{tikzpicture}[auto,node distance=1cm,thick,main node/.style={circle,fill=blue!20,draw,font=\sffamily\Large\bfseries}, scale=0.8]
          \node[main node ] (A) at (0,5) {A};
          \node[main node ] (B) at (2,6.5) {B};
          \node[main node] (E) at (4,5) {E};
          \node[main node] (C) at (0.5,3) {C};
          \node[main node] (D) at (3.5,3) {D};
        
          \path [->] (A) edge node { \tiny $-1.11$} (B);
          \path [->] (B) edge node { \tiny $2.11$} (E);
          \path [->] (A) edge node { \tiny $2.11$} (D);
          \path [->] (A) edge node {$4$} (C);
          \path [->] (B) edge node { \tiny$2.11$} (C);
          \path [->] let \p1=($(D)-(B)$),\n1={atan2(\y1,\x1)},\n2={180+\n1} in
           ($ (B.\n1)!2pt!90:(D.\n2) $) edge node { \tiny $2.11$} ($ (D.\n2)!2pt!-90:(B.\n1) $);
          \path [->] let \p1=($(A)-(E)$),\n1={atan2(\y1,\x1)},\n2={180+\n1} in
           ($ (E.\n1)!2pt!90:(A.\n2) $) edge node { \tiny $2.11$} ($ (A.\n2)!2pt!-90:(E.\n1) $);
        
          \path [->] let \p1=($(B)-(D)$),\n1={atan2(\y1,\x1)},\n2={180+\n1} in
           ($ (D.\n1)!2pt!90:(B.\n2) $) edge node { \tiny $-2.11$} ($ (B.\n2)!2pt!-90:(D.\n1) $);
        
           \path [->] let \p1=($(C)-(E)$),\n1={atan2(\y1,\x1)},\n2={180+\n1} in
           ($ (E.\n1)!2pt!90:(C.\n2) $) edge node { \tiny $-2.11$} ($ (C.\n2)!2pt!-90:(E.\n1) $);
           \path [->] let \p1=($(E)-(C)$),\n1={atan2(\y1,\x1)},\n2={180+\n1} in
           ($ (C.\n1)!2pt!90:(E.\n2) $) edge node { \tiny $-1.11$} ($ (E.\n2)!2pt!-90:(C.\n1) $);
        
          \path [->,bend left=10] (D) edge node {\tiny $5$} (C);
          \path [->,bend left=10] (C) edge node {$-5$} (D);
          \path [->] (E) edge node {$-3$} (D);
          \end{tikzpicture}
          \begin{tikzpicture}[auto,node distance=1cm,thick,main node/.style={circle,fill=blue!20,draw,font=\sffamily\Large\bfseries}, scale=0.8]
            \node[main node ] (A) at (0,5) {A};
            \node[main node ] (B) at (2,6.5) {B};
            \node[main node] (E) at (4,5) {E};
            \node[main node] (C) at (0.5,3) {C};
            \node[main node] (D) at (3.5,3) {D};
          
            \path [->] (A) edge node { \tiny $-1.11$} (B);
            \path [->] (B) edge node { \tiny $2.11$} (E);
            \path [->] (A) edge node { \tiny $2.11$} (D);
            \path [->] (A) edge node {$4$} (C);
            \path [->] (B) edge node { \tiny$2.11$} (C);
            \path [->] let \p1=($(D)-(B)$),\n1={atan2(\y1,\x1)},\n2={180+\n1} in
             ($ (B.\n1)!2pt!90:(D.\n2) $) edge node { \tiny $2.11$} ($ (D.\n2)!2pt!-90:(B.\n1) $);
            \path [->] let \p1=($(A)-(E)$),\n1={atan2(\y1,\x1)},\n2={180+\n1} in
             ($ (E.\n1)!2pt!90:(A.\n2) $) edge node { \tiny $2.11$} ($ (A.\n2)!2pt!-90:(E.\n1) $);
          
            \path [->] let \p1=($(B)-(D)$),\n1={atan2(\y1,\x1)},\n2={180+\n1} in
             ($ (D.\n1)!2pt!90:(B.\n2) $) edge node { \tiny $-2.11$} ($ (B.\n2)!2pt!-90:(D.\n1) $);
          
             \path [->] let \p1=($(C)-(E)$),\n1={atan2(\y1,\x1)},\n2={180+\n1} in
             ($ (E.\n1)!2pt!90:(C.\n2) $) edge node { \tiny $-2.11$} ($ (C.\n2)!2pt!-90:(E.\n1) $);
             \path [->] let \p1=($(E)-(C)$),\n1={atan2(\y1,\x1)},\n2={180+\n1} in
             ($ (C.\n1)!2pt!90:(E.\n2) $) edge node { \tiny $-1.11$} ($ (E.\n2)!2pt!-90:(C.\n1) $);
          
            \path [->,bend left=10] (D) edge node {\tiny $5$} (C);
            \path [->,bend left=10] (C) edge node {$-5$} (D);
            \path [->] (E) edge node {$-3$} (D);
            \end{tikzpicture}
    \caption{jiaqing}
  \end{figure}
\end{center}






\subsection{Feature Augmentation Results}

\begin{table*}[!h]
  \caption{Feature Augmentation Results on Planetoid Datasets (bins = 6)}
  \label{tab:commands}
  \begin{tabular}{*{13}{c}} \toprule
{Aim}  & \multicolumn{4}{c}{{\sc Proteins}} & \multicolumn{4}{c}{{\sc Enzymes}} & \multicolumn{4}{c}{{\sc NCI1}}\\
\cmidrule(lr){2-5}\cmidrule(lr){6-9}\cmidrule(lr){10-13}
& GCN & GIN & SAGE & GAT & GCN & GIN & SAGE & GAT & GCN & GIN & SAGE & GAT \\ \hline
{1 -> 2} & 0.560 & \B 0.662 & 0.469 & 0.469 & 0.630 & \B 0.881 & 0.418 & 0.418 & 0.833 & \B 0.916 & 0.406 & 0.401\\
{1 -> 3} & 0.299 & \B 0.436 & 0.202 & 0.202 & 0.315 & \B 0.344 & 0.262 & 0.261 & / & / & / & / \\
{1 -> 4} & 0.645 & \B 0.648 & 0.170 & 0.170 & 0.597 & \B 0.639 & 0.181 & 0.169 & 0.630 & \B 0.636 & 0.172 & 0.169\\
{1 -> 5} & \B 0.182 & 0.171 & 0.171 & 0.171 & 0.169 & \B 0.200 & 0.173 & 0.169 & 0.217 & \B 0.223 & 0.172 & 0.170\\
{2 -> 3} & 0.435 & 0.387 & \B 0.454 & 0.312 & 0.397 & 0.359 & \B 0.407 & 0.314 & / & / & / & / \\
{2 -> 4} & 0.622 & 0.699 & \B 0.722 & 0.239 & 0.623 & 0.679 & \B 0.702 & 0.249 & 0.689 & 0.705 & \B 0.740 & 0.304\\
{2 -> 5} & \B 0.212 & 0.184 & 0.200 & 0.171 & 0.233 & 0.216 & \B 0.245 & 0.173 & \B 0.237 & 0.222 & 0.235 & 0.177\\
{3 -> 2} & 0.537 & \B 0.652 & 0.482 & 0.469 & 0.569 & \B 0.788 & 0.544 & 0.418 & / & / & / & / \\
{3 -> 4} & \B 0.575 & 0.465 & 0.467 & 0.549 & \B 0.539 & 0.395 & 0.369 & 0.198 & / & / & / & / \\
{3 -> 5} & 0.227 & 0.196 & \B 0.254 & 0.228 & 0.197 & 0.169 & \B 0.203 & 0.169 & / & / & / & / \\	
{4 -> 2} & 0.430 & \B 0.662 & 0.446 & 0.469 & 0.240 & 0.336 & \B 0.458 & 0.418 & 0.100 & \B 0.827 & 0.417 & 0.341\\
{4 -> 3} & 0.247 & \B 0.330 & 0.278 & 0.219 & 0.269 & \B 0.305 & 0.270 & 0.262 & / & / & / & / \\
{4 -> 5} & 0.171 & \B 0.172 & 0.171 & 0.171 & 0.169 & \B 0.180 & 0.171 & 0.168 & 0.169 & \B 0.339 & 0.173 & 0.170\\
{5 -> 2} & 0.522 & \B 0.657 & 0.482 & 0.469 & 0.572 & \B 0.881 & 0.484 & 0.418 & 0.784 & \B 0.919 & 0.836 & 0.401\\
{5 -> 3} & 0.276 & 0.258 & \B 0.328 & 0.202 & 0.283 & 0.308 & \B 0.354 & 0.262 & / & / & / & / \\
{5 -> 4} & \B 0.579 & 0.453 & 0.395 & 0.170 & \B 0.542 & 0.430 & 0.360 & 0.170 & \B 0.629 & 0.534 & 0.588 & 0.180\\
\bottomrule
  \end{tabular}
\end{table*}




\subsubsection{hyper-parameter tunning}

We also compare the performance between node datasets (for node classifications) and graph datasets (for graph classifications).

We also compare the performance between node datasets (for node classifications) and graph datasets (for graph classifications).

We also compare the performance between node datasets (for node classifications) and graph datasets (for graph classifications).



We also compare the performance between node datasets (for node classifications) and graph datasets (for graph classifications).


We also compare the performance between node datasets (for node classifications) and graph datasets (for graph classifications).


We also compare the performance between node datasets (for node classifications) and graph datasets (for graph classifications).


\begin{table}[h]
  \caption{Hyper-parameter : number of bins on node datasets}
  \label{tab:commands}
  \begin{tabular}{*{7}{c}} \toprule
{Bins}  & \multicolumn{3}{c}{{\sc Citeseer}} & \multicolumn{3}{c}{{\sc PubMed}} \\
\cmidrule(lr){2-4}\cmidrule(lr){5-7}
& 3->2 & 4->5 & 5->3 & 3->2 & 4->5 & 5->3 \\ \hline
2 & 1.000& 0.752 & 0.838 & 1.000  & 0.868 & 0.894\\
3 & 0.999& 0.684 & 0.781 & 1.000  & 0.695 & 0.853\\
4 & 1.000& 0.570 & 0.752& 0.999 & 0.618 & 0.833\\
5 & 0.999& 0.491 & 0.723& 1.000  & 0.488 & 0.817\\
6 & 1.000& 0.491 & 0.719& 1.000 & 0.423 & 0.805\\
7 & 1.000& 0.360 & 0.712& 1.000  & 0.345 & 0.799\\
8 & 1.000& 0.386 & 0.703& 0.999 & 0.335 & 0.794 \\
9 & 1.000& 0.373 & 0.696& 0.998  & 0.272 & 0.794\\
10 & 1.000& 0.369 & 0.656& 0.988 &0.262 & 0.792\\
\bottomrule
  \end{tabular}
\end{table}
\begin{table}[htb]
  \caption{Hyper-parameter : number of bins on graph datasets}
  \label{tab:commands}
  \begin{tabular}{*{7}{c}} \toprule
{Bins}  & \multicolumn{3}{c}{{\sc Enzymes}} & \multicolumn{3}{c}{{\sc Proteins}} \\
\cmidrule(lr){2-4}\cmidrule(lr){5-7}
& 3->2 & 4->5 & 5->3 & 3->2 & 4->5 & 5->3 \\ \hline
2 & 0.881 & 0.540 & 0.550 & 0.891 & 0.515 & 0.525\\
3 & 0.869 & 0.356 & 0.377 & 1.000 & 0.340 & 0.418\\
4 & 0.861 & 0.260 & 0.307 & 0.792 & 0.253 & 0.340\\
5 & 0.590 & 0.202 & 0.285 & 0.775 & 0.205 & 0.284\\
6 & 0.696 & 0.174 & 0.274 & 0.662 & 0.171 & 0.232\\
7 & 0.505 & 0.144 & 0.251 & 0.601 & 0.148 & 0.289\\
8 & 0.525 & 0.127 & 0.241 & 0.389 & 0.128 & 0.266\\
9 & 0.408 & 0.115 & 0.238 & 0.437 & 0.121 & 0.269\\
10 & 0.510 & 0.107 & 0.238 & 0.335 & 0.108 & 0.257\\

\bottomrule
  \end{tabular}
\end{table}

\subsubsection{graph embedding vs linear embedding}





  \begin{figure}[h]
  \centering
  \begin{center}
  \hspace*{-1in}
      \includegraphics[width=0.36\linewidth]{fig/Cora_1to3_tSNE.eps}
      \includegraphics[width=0.36\linewidth]{fig/Citeseer_1to3_tSNE.eps}
      \includegraphics[width=0.36\linewidth]{fig/PubMed_1to3_tSNE.eps}
      \hspace*{-1in}

      \caption{tSNE on graph embeddings with Degree predicting PageRank}
    \hspace*{-1.5in}
      \includegraphics[width=0.35\linewidth]{fig/Cora_2to1_tSNE.eps}
      \includegraphics[width=0.35\linewidth]{fig/Citeseer_2to1_tSNE.eps}
      \includegraphics[width=0.35\linewidth]{fig/PubMed_2to1_tSNE.eps}
      \hspace*{-1.5in}

      \caption{tSNE on graph embeddings with Clustering predicting Degree}
  \end{center}
  \end{figure}









\subsection{Additional Features}



\subsection{self-generated graphs}

\paragraph{}

\subsection{Regression tasks}







\subsection{Comparison between datasets}



\section{Conclusions and Discussions}




\paragraph{Future works}







%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

%%
%% If your work has an appendix, this is the place to put it.
\appendix

\section{niubi}

\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.
